{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional , GRU ,LSTM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding , Convolution2D\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import theano.ifelse\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('C:/Users/hp/Desktop/IIITD/Medical Information Extraction/train_medical.csv')\n",
    "test_data = pd.read_csv('C:/Users/hp/Desktop/IIITD/Medical Information Extraction/test_medical.csv')\n",
    "validation_data = pd.read_csv('C:/Users/hp/Desktop/IIITD/Medical Information Extraction/validation_medical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#frames = [train_data , validation_data]\n",
    "#train_data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['_unit_id', '_created_at', '_canary', '_id', '_started_at', '_channel',\n",
    "       '_trust', '_worker_id', '_country', '_region', '_city', '_ip',\n",
    "       'direction', 'b1', 'b2', 'direction_gold', 'e1', 'e2',\n",
    "       'relex_relcos', 'sent_id', 'twrex', 'term1' , 'term2']\n",
    "\n",
    "\n",
    "train_data = train_data.drop( columns_to_drop , axis = 1)\n",
    "test_data = test_data.drop(columns_to_drop , axis = 1)\n",
    "validation_data = validation_data.drop(columns_to_drop , axis = 1)\n",
    "\n",
    "train_data.loc[train_data.relation == 'is location of' , 'relation'] = 'Others'\n",
    "train_data.loc[train_data.relation == 'diagnosed by' , 'relation'] = 'Others'\n",
    "train_data.loc[train_data.relation == 'contraindicates' , 'relation'] = 'Others'\n",
    "train_data.loc[train_data.relation == 'location' , 'relation'] = 'Others'\n",
    "train_data.loc[train_data.relation == 'location of' , 'relation'] = 'Others'\n",
    "train_data.loc[train_data.relation == 'diagnose_by_test_or_drug' , 'relation'] = 'Others'\n",
    "train_data.loc[train_data.relation == 'is diagnosed by' , 'relation'] = 'Others'\n",
    "\n",
    "\n",
    "test_data.loc[test_data.relation == 'is location of' , 'relation'] = 'Others'\n",
    "test_data.loc[test_data.relation == 'diagnosed by' , 'relation'] = 'Others'\n",
    "test_data.loc[test_data.relation == 'contraindicates' , 'relation'] = 'Others'\n",
    "test_data.loc[test_data.relation == 'location' , 'relation'] = 'Others'\n",
    "test_data.loc[test_data.relation == 'location of' , 'relation'] = 'Others'\n",
    "test_data.loc[test_data.relation == 'diagnose_by_test_or_drug' , 'relation'] = 'Others'\n",
    "test_data.loc[test_data.relation == 'is diagnosed by' , 'relation'] = 'Others'\n",
    "\n",
    "validation_data.loc[validation_data.relation == 'is location of' , 'relation'] = 'Others'\n",
    "validation_data.loc[validation_data.relation == 'diagnosed by' , 'relation'] = 'Others'\n",
    "validation_data.loc[validation_data.relation == 'contraindicates' , 'relation'] = 'Others'\n",
    "validation_data.loc[validation_data.relation == 'location' , 'relation'] = 'Others'\n",
    "validation_data.loc[validation_data.relation == 'location of' , 'relation'] = 'Others'\n",
    "validation_data.loc[validation_data.relation == 'diagnose_by_test_or_drug' , 'relation'] = 'Others'\n",
    "validation_data.loc[validation_data.relation == 'is diagnosed by' , 'relation'] = 'Others'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13340, 2) (4566, 2) (4270, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape , test_data.shape , validation_data.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "\n",
    "def text_prepare(doc):\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    doc = REPLACE_BY_SPACE_RE.sub(' ' , doc)\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_clean = [ text_prepare(x) for x in train_data.sentence]\n",
    "validation_data_clean = [ text_prepare(x) for x in validation_data.sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_clean = [ text_prepare(x) for x in test_data.sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenising "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7835"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_docs_train = tokenizer.texts_to_sequences(train_data_clean)\n",
    "encoded_docs_validation = tokenizer.texts_to_sequences(validation_data_clean)\n",
    "encoded_docs_test = tokenizer.texts_to_sequences(test_data_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "# calculate max document length\n",
    "lengths = [len(s.split()) for s in train_data_clean]\n",
    "max_length = max(lengths)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########padding\n",
    "max_length = 77\n",
    "padded_docs_train = pad_sequences(encoded_docs_train, maxlen = max_length, padding='post')\n",
    "padded_docs_validation = pad_sequences(encoded_docs_validation, maxlen = max_length, padding='post')\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen = max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = padded_docs_train\n",
    "X_val = padded_docs_validation\n",
    "X_test = padded_docs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13340, 77) (4566, 77) (4270, 77)\n"
     ]
    }
   ],
   "source": [
    "print( X_train.shape , X_test.shape , X_val.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13340,) (4566,) (4270,)\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_data['relation'])\n",
    "y_train = encoder.transform(train_data['relation'])\n",
    "y_val = encoder.transform(validation_data['relation'])\n",
    "y_test = encoder.transform(test_data['relation'])\n",
    "\n",
    "\n",
    "#y_train_cat = to_categorical(y_train)\n",
    "#y_test_cat = to_categorical(y_test)\n",
    "\n",
    "print(y_train.shape , y_test.shape , y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename_pmc = 'C:/Users/hp/Word_embeddings/PubMed-and-PMC-w2v.bin'\n",
    "model_pmc = gensim.models.KeyedVectors.load_word2vec_format( filename_pmc  , binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM_PMC = 200\n",
    "vocabulary_size =  vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix_pmc = np.zeros(( vocabulary_size , EMBEDDING_DIM_PMC ))\n",
    "for word , i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector_pmc = model_pmc[word]\n",
    "    except KeyError:\n",
    "        embedding_vector_pmc = None\n",
    "    if embedding_vector_pmc is not None:\n",
    "        embedding_matrix_pmc[i] = embedding_vector_pmc\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model ( Attention over GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "        \n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "     Only computes a batch-wise average of precision.\n",
    "     Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "     Only computes a batch-wise average of recall.\n",
    "     Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13340, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_cat = to_categorical(y_train)\n",
    "Y_train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4270, 3)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val_cat = to_categorical(y_val)\n",
    "Y_val_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4566, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_cat = to_categorical(y_test) \n",
    "y_test_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding( vocab_size, 200 ,\n",
    "          weights=[embedding_matrix_pmc], input_length= 77, trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(GRU( 100 ,return_sequences= True, dropout=0.3, recurrent_dropout=0.25)))\n",
    "model.add(Attention())\n",
    "model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['acc' , precision , recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 77, 200)           1567000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 77, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 77, 200)           180600    \n",
      "_________________________________________________________________\n",
      "attention_3 (Attention)      (None, 200)               277       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 603       \n",
      "=================================================================\n",
      "Total params: 1,748,480.0\n",
      "Trainable params: 181,480.0\n",
      "Non-trainable params: 1,567,000.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13340 samples, validate on 4270 samples\n",
      "Epoch 1/10\n",
      "13340/13340 [==============================] - 97s - loss: 0.9420 - acc: 0.5981 - precision: 0.6434 - recall: 0.1735 - val_loss: 0.8582 - val_acc: 0.6302 - val_precision: 0.6455 - val_recall: 0.4761\n",
      "Epoch 2/10\n",
      "13340/13340 [==============================] - 98s - loss: 0.7144 - acc: 0.7355 - precision: 0.7713 - recall: 0.6752 - val_loss: 0.7320 - val_acc: 0.7166 - val_precision: 0.7332 - val_recall: 0.6686\n",
      "Epoch 3/10\n",
      "13340/13340 [==============================] - 98s - loss: 0.6257 - acc: 0.7588 - precision: 0.7890 - recall: 0.7136 - val_loss: 0.6911 - val_acc: 0.7101 - val_precision: 0.7493 - val_recall: 0.6489\n",
      "Epoch 4/10\n",
      "13340/13340 [==============================] - 99s - loss: 0.5895 - acc: 0.7698 - precision: 0.8046 - recall: 0.7214 - val_loss: 0.6586 - val_acc: 0.7152 - val_precision: 0.7603 - val_recall: 0.6726\n",
      "Epoch 5/10\n",
      "13340/13340 [==============================] - 101s - loss: 0.5642 - acc: 0.7780 - precision: 0.8075 - recall: 0.7385 - val_loss: 0.6473 - val_acc: 0.7218 - val_precision: 0.7663 - val_recall: 0.6761\n",
      "Epoch 6/10\n",
      "13340/13340 [==============================] - 96s - loss: 0.5496 - acc: 0.7797 - precision: 0.8064 - recall: 0.7416 - val_loss: 0.6327 - val_acc: 0.7314 - val_precision: 0.7771 - val_recall: 0.6808\n",
      "Epoch 7/10\n",
      "13340/13340 [==============================] - 100s - loss: 0.5310 - acc: 0.7891 - precision: 0.8168 - recall: 0.7478 - val_loss: 0.6258 - val_acc: 0.7265 - val_precision: 0.7706 - val_recall: 0.6808\n",
      "Epoch 8/10\n",
      "13340/13340 [==============================] - 102s - loss: 0.5215 - acc: 0.7905 - precision: 0.8160 - recall: 0.7524 - val_loss: 0.6142 - val_acc: 0.7307 - val_precision: 0.7725 - val_recall: 0.6857\n",
      "Epoch 9/10\n",
      "13340/13340 [==============================] - 101s - loss: 0.5117 - acc: 0.7930 - precision: 0.8180 - recall: 0.7579 - val_loss: 0.6159 - val_acc: 0.7368 - val_precision: 0.7703 - val_recall: 0.6913\n",
      "Epoch 10/10\n",
      "13340/13340 [==============================] - 100s - loss: 0.4972 - acc: 0.8028 - precision: 0.8271 - recall: 0.7696 - val_loss: 0.6145 - val_acc: 0.7379 - val_precision: 0.7728 - val_recall: 0.7023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29b2365da20>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train , Y_train_cat , epochs = 10, batch_size = 1000 ,validation_data = ( X_val ,Y_val_cat ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4566/4566 [==============================] - 11s    \n",
      "acc : 73.00%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test_cat)\n",
    "print(\"%s : %.2f%%\" % (model.metrics_names[1] , scores[1]*100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision : 75.35%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s : %.2f%%\" % (model.metrics_names[2] , scores[2]*100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall : 69.25%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s : %.2f%%\" % (model.metrics_names[3] , scores[3]*100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision = scores[2]*100\n",
    "recall = scores[3]*100\n",
    "## f1 score\n",
    "f1_score = (2 * precision * recall )/( recall + precision )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.1724162566137"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
